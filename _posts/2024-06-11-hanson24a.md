---
title: Rademacher Complexity of Neural ODEs via Chen-Fliess Series
abstract: We show how continuous-depth neural ODE models can be framed as single-layer,
  infinite-width nets using the Chen-Fliess series expansion for nonlinear ODEs. In
  this net, the output “weights” are taken from the signature of the control input
  — a tool used to represent infinite-dimensional paths as a sequence of tensors —
  which comprises iterated integrals of the control input over a simplex. The “features”
  are taken to be iterated Lie derivatives of the output function with respect to
  the vector fields in the controlled ODE model. The main result of this work applies
  this framework to derive compact expressions for the Rademacher complexity of ODE
  models that map an initial condition to a scalar output at some terminal time. The
  result leverages the straightforward analysis afforded by single-layer architectures.
  We conclude with some examples instantiating the bound for some specific systems
  and discuss potential follow-up work.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hanson24a
month: 0
tex_title: "{Rademacher Complexity of Neural ODEs via Chen-Fliess Series}"
firstpage: 1
lastpage: 12
page: 1-12
order: 1
cycles: false
bibtex_author: Hanson, Joshua and Raginsky, Maxim
author:
- given: Joshua
  family: Hanson
- given: Maxim
  family: Raginsky
date: 2024-06-11
address:
container-title: Proceedings of the 6th Annual Learning for Dynamics & Control Conference
volume: '242'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 11
pdf: https://proceedings.mlr.press/v242/hanson24a/hanson24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---

---
title: Soft convex quantization: revisiting Vector Quantization with convex optimization
abstract: 'Vector Quantization (VQ) is a well-known technique in deep learning for
  extracting informative discrete latent representations. VQ-embedded models have
  shown impressive results in a range of applications including image and speech generation.
  VQ operates as a parametric K-means algorithm that quantizes inputs using a single
  codebook vector in the forward pass. While powerful, this technique faces practical
  challenges including codebook collapse, non-differentiability and lossy compression.
  To mitigate the aforementioned issues, we propose Soft Convex Quantization (SCQ)
  as a direct substitute for VQ. SCQ works like a differentiable convex optimization
  (DCO) layer: in the forward pass, we solve for the optimal convex combination of
  codebook vectors to quantize the inputs. In the backward pass, we leverage differentiability
  through the optimality conditions of the forward solution. We then introduce a scalable
  relaxation of the SCQ optimization and demonstrate its efficacy on the CIFAR-10,
  GTSRB and LSUN datasets. We train powerful SCQ autoencoder models that significantly
  outperform matched VQ architectures, observing an order of magnitude better image
  reconstruction and codebook usage with comparable quantization runtime.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gautam24a
month: 0
tex_title: "Soft convex quantization: revisiting {V}ector {Q}uantization with convex
  optimization"
firstpage: 273
lastpage: 285
page: 273-285
order: 1
cycles: false
bibtex_author: Gautam, Tanmay and Pryzant, Reid and Yang, Ziyi and Zhu, Chenguang
  and Sojoudi, Somayeh
author:
- given: Tanmay
  family: Gautam
- given: Reid
  family: Pryzant
- given: Ziyi
  family: Yang
- given: Chenguang
  family: Zhu
- given: Somayeh
  family: Sojoudi
date: 2024-06-11
address:
container-title: Proceedings of the 6th Annual Learning for Dynamics & Control Conference
volume: '242'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 11
pdf: https://proceedings.mlr.press/v242/gautam24a/gautam24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---

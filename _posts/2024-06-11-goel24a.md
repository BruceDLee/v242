---
title: Can a transformer represent a Kalman filter?
abstract: Transformers are a class of autoregressive deep learning architectures which
  have recently achieved state-of-the-art performance in various vision, language,
  and robotics tasks. We revisit the problem of Kalman Filtering in linear dynamical
  systems and show that Transformers can approximate the Kalman Filter in a strong
  sense. Specifically, for any observable LTI system we construct an explicit causally-masked
  Transformer which implements the Kalman Filter, up to a small additive error which
  is bounded uniformly in time; we call our construction the Transformer Filter. Our
  construction is based on a two-step reduction. We first show that a softmax self-attention
  block can exactly represent a Nadarayaâ€“Watson kernel smoothing estimator with a
  Gaussian kernel. We then show that this estimator closely approximates the Kalman
  Filter. We also investigate how the Transformer Filter can be used for measurement-feedback
  control and prove that the resulting nonlinear controllers closely approximate the
  performance of standard optimal control policies such as the LQG controller.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: goel24a
month: 0
tex_title: "Can a transformer represent a {K}alman filter?"
firstpage: 1502
lastpage: 1512
page: 1502-1512
order: 1
cycles: false
bibtex_author: Goel, Gautam and Bartlett, Peter
author:
- given: Gautam
  family: Goel
- given: Peter
  family: Bartlett
date: 2024-06-11
address:
container-title: Proceedings of the 6th Annual Learning for Dynamics & Control Conference
volume: '242'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 11
pdf: https://proceedings.mlr.press/v242/goel24a/goel24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---

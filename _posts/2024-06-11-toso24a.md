---
title: 'Meta-Learning Linear Quadratic Regulators: A Policy Gradient MAML Approach
  for Model-free LQR'
booktitle: Proceedings of the 6th Annual Learning for Dynamics \& Control Conference
abstract: We investigate the problem of learning linear quadratic regulators (LQR)
  in a multi-task, heterogeneous, and model-free setting. We characterize the stability
  and personalization guarantees of a policy gradient-based (PG) model-agnostic meta-learning
  (MAML) (Finn et al., 2017) approach for the LQR problem under different task-heterogeneity
  settings. We show that our MAML-LQR algorithm produces a stabilizing controller
  close to each task-specific optimal controller up to a task-heterogeneity bias in
  both model-based and model-free learning scenarios. Moreover, in the model-based
  setting, we show that such a controller is achieved with a linear convergence rate,
  which improves upon sub-linear rates from existing work. Our theoretical guarantees
  demonstrate that the learned controller can efficiently adapt to unseen LQR tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: toso24a
month: 0
tex_title: "{Meta-Learning Linear Quadratic Regulators: A Policy Gradient MAML Approach
  for Model-free LQR}"
firstpage: 1
lastpage: 14
page: 1-14
order: 1
cycles: false
bibtex_author: Toso, Leonardo Felipe and Zhan, Donglin and Anderson, James and Wang,
  Han
author:
- given: Leonardo Felipe
  family: Toso
- given: Donglin
  family: Zhan
- given: James
  family: Anderson
- given: Han
  family: Wang
date: 2024-06-11
address:
container-title: Proceedings of the 6th Annual Learning for Dynamics \& Control Conference
volume: '242'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 11
pdf: https://proceedings.mlr.press/v242/toso24a/toso24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---

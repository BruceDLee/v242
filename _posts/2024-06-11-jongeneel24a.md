---
title: A large deviations perspective on policy gradient algorithms
abstract: Motivated by policy gradient methods in the context of reinforcement learning,
  we derive the first large deviation rate function for the iterates generated by
  stochastic gradient descent for possibly non-convex objectives satisfying a Polyak-≈Åojasiewicz
  condition. Leveraging the contraction principle from large deviations theory, we
  illustrate the potential of this result by showing how convergence properties of
  policy gradient with a softmax parametrization and an entropy regularized objective
  can be naturally extended to a wide spectrum of other policy parametrizations.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jongeneel24a
month: 0
tex_title: "A large deviations perspective on policy gradient algorithms"
firstpage: 916
lastpage: 928
page: 916-928
order: 916
cycles: false
bibtex_author: Jongeneel, Wouter and Kuhn, Daniel and Li, Mengmeng
author:
- given: Wouter
  family: Jongeneel
- given: Daniel
  family: Kuhn
- given: Mengmeng
  family: Li
date: 2024-06-11
address:
container-title: Proceedings of the 6th Annual Learning for Dynamics & Control Conference
volume: '242'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 11
pdf: https://proceedings.mlr.press/v242/jongeneel24a/jongeneel24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---

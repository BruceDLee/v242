---
title: 'On the convergence of adaptive first order methods: proximal gradient and
  alternating minimization algorithms'
booktitle: Proceedings of the 6th Annual Learning for Dynamics \& Control Conference
abstract: Building upon recent works on linesearch-free adaptive proximal gradient
  methods, this paper proposes AdaPG, a framework that unifies and extends existing
  results by providing larger stepsize policies and improved lower bounds. Different
  choices of the parameters are discussed and the efficacy of the resulting methods
  is demonstrated through numerical simulations. In an attempt to better understand
  the underlying theory, its convergence is established in a more general setting
  that allows for time-varying parameters. Finally, an adaptive alternating minimization
  algorithm is presented by exploring the dual setting. This algorithm not only incorporates
  additional adaptivity but also expands its applicability beyond standard strongly
  convex settings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: latafat24a
month: 0
tex_title: "{On the convergence of adaptive first order methods: proximal gradient
  and alternating minimization algorithms}"
firstpage: 1
lastpage: 12
page: 1-12
order: 1
cycles: false
bibtex_author: Latafat, Puya and Themelis, Andreas and Patrinos, Panagiotis
author:
- given: Puya
  family: Latafat
- given: Andreas
  family: Themelis
- given: Panagiotis
  family: Patrinos
date: 2024-06-11
address:
container-title: Proceedings of the 6th Annual Learning for Dynamics \& Control Conference
volume: '242'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 11
pdf: https://proceedings.mlr.press/v242/latafat24a/latafat24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
